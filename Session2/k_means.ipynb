{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50b4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620aa87",
   "metadata": {},
   "source": [
    "# 1. Class Cluster & Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38741e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Member store info of data point: tf-idf, news group of d, file name of d.\n",
    "\n",
    "class Member():\n",
    "    def __init__(self, r_d, label = None, doc_id = None):\n",
    "        self._r_d = r_d\n",
    "        self._label = label\n",
    "        self._doc_id = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14325eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster store the data points\n",
    "\n",
    "class Cluster():\n",
    "    def __init__(self):\n",
    "    # The underscore prefix is meant as a hint that a variable or method starting with \"_\" is intended for internal use\n",
    "        self._centroid = None\n",
    "        self._members = []\n",
    "    \n",
    "    # Reset members (data points) in the list of centroids\n",
    "    def reset_members(self):\n",
    "        self._members = []\n",
    "        \n",
    "    # Set centroid\n",
    "    def set_centroid(self, new_centroid):\n",
    "        self._centroid = new_centroid\n",
    "    \n",
    "    def add_member(self, new_member):\n",
    "        self._members.append(new_member)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b931a2c",
   "metadata": {},
   "source": [
    "# 2. Class K - Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a678d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans():\n",
    "    # Init with num_cluster, create K list store member of K centroid\n",
    "    def __init__(self, num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = []\n",
    "        for i in range(self._num_clusters):\n",
    "            self._clusters.append(Cluster())\n",
    "        self._E = [] # list of centroids\n",
    "        self._S = 0 # Overall similarity\n",
    "        \n",
    "    # Load data\n",
    "    def load_data(self, path = \"C:/Users/HH/OneDrive - Hanoi University of Science and Technology/ML basics/Session1/20news-bydate/\"):\n",
    "        # Get data from each line with label, doc_id, index & tfidf of its vocab\n",
    "        def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "            r_d = [0.0 for i in range(vocab_size)]\n",
    "            \n",
    "            # Split space & : in context data of each line \n",
    "            # Get index (id vocal of each line) & tfidfs\n",
    "            indices_and_tfidfs = sparse_r_d.split()\n",
    "            for index_and_tfidf in indices_and_tfidfs:\n",
    "                index = int(index_and_tfidf.split(':')[0])\n",
    "                tfidf = float(index_and_tfidf.split(':')[1])\n",
    "                r_d[index] = tfidf\n",
    "            return np.array(r_d)\n",
    "        \n",
    "        # Open file (newsgroup, id, context)\n",
    "        with open(path + \"data_tf_idf.txt\") as file:\n",
    "            content = file.read().splitlines()\n",
    "        \n",
    "        # Get size of words_idf file (number of words)\n",
    "        with open(path + \"words_idf.txt\") as file:\n",
    "            vocab_size = len(file.read().splitlines())\n",
    "            \n",
    "        self._data = []\n",
    "        for line in content:\n",
    "            data_lines = line.split(\"<fff>\")\n",
    "            r_d = sparse_to_dense(data_lines[2], vocab_size)\n",
    "            self._data.append(Member(r_d = r_d, label = int(data_lines[0]), doc_id = int(data_lines[1])))\n",
    "    \n",
    "    # Random element to init centroid\n",
    "    def random_init(self, seed_value):\n",
    "        random.seed(seed_value)\n",
    "        # Crawl list members\n",
    "        members = [member._r_d for member in self._data]\n",
    "        \n",
    "        # random.choice: random sample from 1-D array\n",
    "        # Same as np.arange without same num in array\n",
    "        pos = np.random.choice(len(self._data), self._num_clusters, replace = False)\n",
    "        centroid = []\n",
    "        for i in pos:\n",
    "            centroid.append(members[i])\n",
    "        # Update centroid\n",
    "        self._E = centroid\n",
    "        for i in range(self._num_clusters):\n",
    "            self._clusters[i].set_centroid(centroid[i])\n",
    "    \n",
    "    # Calculate similarity from centroid to another member\n",
    "    # https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "    def compute_similarity(self, member, centroid):\n",
    "        # K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "        return cosine_similarity([member._r_d], [centroid])\n",
    "\n",
    "    # Assign member to cluster\n",
    "    def select_cluster_for(self, member):\n",
    "        best_fit_cluster = None\n",
    "        \n",
    "        # min(cos) = -1\n",
    "        max_similarity = -1\n",
    "        \n",
    "        for cluster in self._clusters:\n",
    "            similarity = self.compute_similarity(member, cluster._centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = similarity\n",
    "        best_fit_cluster.add_member(member)\n",
    "        return max_similarity\n",
    "    \n",
    "    # Recompute centroid\n",
    "    def update_centroid_of(self, cluster):\n",
    "        # list of all tf-idf representation\n",
    "        member_r_d = [member._r_d for member in cluster._members]\n",
    "        avg_r_d = np.mean(member_r_d, axis = 0)\n",
    "        sqrt_sum = np.sqrt(np.sum(avg_r_d**2))\n",
    "        new_centroid = avg_r_d * 1./sqrt_sum\n",
    "        \n",
    "        # update centroid\n",
    "        cluster._centroid = new_centroid\n",
    "        \n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        criteria = [\"centroid\", \"similarity\", \"max_iters\"]\n",
    "        assert criterion in criteria\n",
    "        if criterion == \"max_iters\":\n",
    "            if self._iteration >= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        if criterion == \"centroid\":\n",
    "            E_new = [list(cluster._centroid) for cluster in self._clusters]\n",
    "            E_new_minus_E = [centroid for centroid in E_new if centroid not in self._E]\n",
    "            if len(E_new_minus_E) <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        if criterion == \"similarity\":\n",
    "            new_S_minus_S = self._new_S - self._S\n",
    "            self._S = self._new_S\n",
    "            if new_S_minus_S <= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    # Loop update cluster until convergence \n",
    "    # Step 1: For each instance, assign it to nearest centroid\n",
    "    # Step 2: For each cluster, recompute its centroid\n",
    "    def run(self, seed_value, criterion, threshold):\n",
    "        self.random_init(seed_value)\n",
    "        self._iteration = 0\n",
    "        \n",
    "        while True:\n",
    "            # reset clusters, retain only centroids\n",
    "            for cluster in self._clusters:\n",
    "                cluster.reset_members()\n",
    "                self._new_S = 0\n",
    "            # select nearest member for each cluster\n",
    "            for member in self._data:\n",
    "                max_s = self.select_cluster_for(member)\n",
    "                self._new_S += max_s\n",
    "            # recompute the centroids\n",
    "            for cluster in self._clusters:\n",
    "                self.update_centroid_of(cluster)\n",
    "            # stop loop by stopping condition\n",
    "            self._iteration += 1\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                break\n",
    "    def compute_purity(self):\n",
    "        majority_sum = 0\n",
    "        for cluster in self._clusters:\n",
    "            member_label = [member._label for member in cluster._members]\n",
    "            max_label = max([member_label.count(label) for label in range(20)])\n",
    "            majority_sum += max_label\n",
    "        return majority* 1./len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d049ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = Kmeans(num_clusters=8)\n",
    "kmeans.load_data()\n",
    "kmeans.run(seed_value = 42, criterion='similarity', threshold = 1e-3)\n",
    "print(kmeans.compute_purity())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
