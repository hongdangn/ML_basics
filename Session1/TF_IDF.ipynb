{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50208bc5",
   "metadata": {},
   "source": [
    "# Import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a94020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5712f68",
   "metadata": {},
   "source": [
    "## Step 1: Collect data and write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e09b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/HH/OneDrive - Hanoi University of Science and Technology/ML basics/Session1/20news-bydate/\"\n",
    "train_dir = path + \"20news-bydate-train\"\n",
    "test_dir = path + \"20news-bydate-test\"\n",
    "train_path = \"20news-bydate-train\"\n",
    "test_path = \"20news-bydate-test\"\n",
    "\n",
    "def gather_20newsgroup_data():\n",
    "    newsgroup_list = []\n",
    "    for newsgroup in os.listdir(train_dir):\n",
    "        newsgroup_list.append(newsgroup)\n",
    "    newsgroup_list.sort()\n",
    "    \n",
    "    with open(\"20news-bydate/stop_words.txt\") as file:\n",
    "        stop_words = file.read().splitlines()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    def collect_data_from(parent_dir, dir_path, newsgroup_list):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "              for filename in os.listdir(parent_dir + '/' + newsgroup + '/'):     \n",
    "                    with open(\"20news-bydate/\" + dir_path + '/' + newsgroup + '/' + filename) as f:\n",
    "                        text = f.read().lower()\n",
    "                    content = str(group_id) + \"<fff>\" + filename + \"<fff>\" \n",
    "                    for word in re.split(\"\\W+\", text):\n",
    "                        if word not in stop_words:\n",
    "                            content += ps.stem(word) + \" \"\n",
    "                    data.append(content)\n",
    "        return data\n",
    "    train_data = collect_data_from(parent_dir = train_dir, dir_path = train_path, newsgroup_list = newsgroup_list)\n",
    "    test_data = collect_data_from(parent_dir = test_dir, dir_path = test_path, newsgroup_list = newsgroup_list)\n",
    "    full_data = train_data + test_data\n",
    "    with open (\"20news-bydate/20news-train-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open (\"20news-bydate/20news-test-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    with open (\"20news-bydate/20news-full-processed.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfe7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_20newsgroup_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0b41e",
   "metadata": {},
   "source": [
    "### Basic knowledge about TF - TDF\n",
    "\n",
    "About TF - TDF, the higher the numerical weight value, the rarer the term. The smaller the weight, the more common the term.\n",
    "\n",
    "$\\star$ The TF(Term Frequency) of a word is the frequency (i.e number of times it appears) of a word in a document, the TF of a word is following the formula:  \n",
    "   $tf(t, d)$ = $\\dfrac{f(t, d)}{max\\left\\{f(t, d) | t \\in D\\right\\}}$, $D$ is the corpus.\n",
    "   \n",
    "$\\star$ The IDF (Inverse Document Frequency) of a word is the measure of how significant that term is in the whole corpus (a body of documents). The IDF of a word is following the formula:\n",
    "   $idf(t, D)$ = $\\log(\\dfrac{N}{N(t)})$, $N$ is the number of documents in the corpus and $N(t)$ is the number of documents in the corpus that the term $t$ appears\n",
    "\n",
    "So, we have $tf-idf(t, d, D) = tf(t, d).idf(t, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c3f3a",
   "metadata": {},
   "source": [
    "## Step 2: Generate dictionary and compute the $idf$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f45eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dictionary(data_path):\n",
    "    def compute_idf(df, corpus_size):\n",
    "        assert df > 0\n",
    "        return np.log10(corpus_size * 1./df)\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    \"\"\" doc_count is the list with keys are distince word in the dictionary and the values is the number of documents\n",
    "    containing that word in the corpus \"\"\"\n",
    "    \n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.split(\"<fff>\")\n",
    "        text = list(set(line[-1].split()))\n",
    "        for word in text:\n",
    "            doc_count[word] += 1\n",
    "    words_idf = []\n",
    "    for word in dict(doc_count):\n",
    "        idf_word = compute_idf(doc_count[word], corpus_size)\n",
    "        line = word + \"<fff>\" + str(idf_word)\n",
    "        words_idf.append(line)\n",
    "    with open(\"20news-bydate/words_idf.txt\", \"w\") as file:\n",
    "        file.write('\\n'.join(words_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5386a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dictionary(\"20news-bydate/20news-full-processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eef457bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tf_idf(datapath):\n",
    "    # dictionary: {word: (idf of that word, the ordered of the word)}\n",
    "    with open(\"20news-bydate/words_idf.txt\") as file:\n",
    "        content = file.read().splitlines()\n",
    "        words_idf = {}\n",
    "        for index, line in enumerate(content):\n",
    "            words_idf[line.split(\"<fff>\")[0]] = (line.split(\"<fff>\")[1], index)\n",
    "            \n",
    "    # list: [(newsgroup_id, filename, all the words)]\n",
    "    with open(datapath) as file:\n",
    "        lines = file.read().splitlines()\n",
    "        documents = []\n",
    "        for line in lines:\n",
    "            documents.append((line.split(\"<fff>\")[0], line.split(\"<fff>\")[1], line.split(\"<fff>\")[2]))\n",
    "    \n",
    "    data_tf_idf = []\n",
    "    for document in documents:\n",
    "        label, filename, text = document\n",
    "        content = text.split()\n",
    "        words = list(set(content))\n",
    "        \n",
    "        # the word with the maximum frequency in content\n",
    "        max_frequency = 0\n",
    "        for word in words:\n",
    "            max_frequency = max(max_frequency, content.count(word))\n",
    "            \n",
    "        line = label + \"<fff>\" + filename + \"<fff>\"\n",
    "        for word in words:\n",
    "             word_tf = content.count(word) * 1./max_frequency\n",
    "             word_tf_idf = word_tf * float(words_idf[word][0])\n",
    "             line += str(words_idf[word][1]) + \": \" + str(word_tf_idf) + \" \"\n",
    "        data_tf_idf.append(line)\n",
    "    with open(\"20news-bydate/data_tf_idf.txt\", \"w\") as file:\n",
    "        file.write('\\n'.join(data_tf_idf))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b28e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf_idf(\"20news-bydate/20news-full-processed.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
